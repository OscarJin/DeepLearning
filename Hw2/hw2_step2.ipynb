{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da49b89-1192-4ad7-8efa-7f38b61fafd6",
   "metadata": {},
   "source": [
    "# Homework 2, Step 2. CNN for CIFAR-100\n",
    "\n",
    "In the step 2, you need to try CNN on the CIFAR-100 classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a41c9-3ac0-4313-92aa-05ad4bb6f961",
   "metadata": {},
   "source": [
    "## 1. Prepare the dataset and the model\n",
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704dfb5f-488d-4d4f-813d-45d52316e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import time\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6aa65-3eeb-4020-82d8-035c0ec29a0f",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "**For step 3, 4, 5,** you need to change the value of momentum, weight decay, data augmentation and batch normalization, to see the difference.\n",
    "\n",
    "`mmt`: momentum for the optimizer. Use `0` if you do not want to use the momentum.\n",
    "\n",
    "`wd`: weight decay for the optimizer. Use `0` if you do not want to use the weight decay.\n",
    "\n",
    "`data_augmentation`: whether to use the data augmentation for the training.\n",
    "\n",
    "`use_BN`: wheter to use the batch normalization for the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a13524-df5b-4a31-9aaa-99b0b899eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05 # learning rate\n",
    "opt = 'sgd'\n",
    "batchsize = 256 # training batchsize\n",
    "\n",
    "mmt = 0.9 # momentum for optimizer\n",
    "wd = 0.001 # weight_decay for optimizer\n",
    "data_augmentation = True\n",
    "use_BN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b09db2-d22c-4915-b642-40416f6622bd",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "In pytorch, you can use the following API to load the dataset.\n",
    "\n",
    "The RGB mean and std are pre-calculated values for normalizing the data. **Do not modify them**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454fbb59-155b-4f57-b834-3eef178ca19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "rgb_mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "rgb_std = np.array([0.2023, 0.1994, 0.2010])\n",
    "if data_augmentation:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(rgb_mean, rgb_std),\n",
    "    ])\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(rgb_mean, rgb_std),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(rgb_mean, rgb_std),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(\n",
    "    root='../data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batchsize, shuffle=True, num_workers=1)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(\n",
    "    root='../data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=500, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de988042-1dff-4cae-ab43-5a190e26fb02",
   "metadata": {},
   "source": [
    "### Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c828fe5d-3889-4262-b5f7-a480201901f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "def compute_feature_map_size(ks, pks):\n",
    "    s = 32\n",
    "    for k, pk in zip(ks, pks):\n",
    "        s = s-k+1\n",
    "        s = int(s/pk)\n",
    "    return s\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_chns, ks, pks):\n",
    "        super(LeNet, self).__init__()\n",
    "        in_chns, out_chns = [3, *hidden_chns[:-1]], hidden_chns\n",
    "        layers = []\n",
    "        for ic, oc, k, pk in zip(in_chns, out_chns, ks, pks):\n",
    "            layers.append(nn.Conv2d(ic, oc, kernel_size=k))\n",
    "            if use_BN:\n",
    "                layers.append(nn.BatchNorm2d(oc))\n",
    "            layers.append(nn.MaxPool2d(kernel_size=pk) if pk>1 else Identity()) \n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        s = compute_feature_map_size(ks, pks)\n",
    "        print(f\"feature size: {s}\")\n",
    "\n",
    "        self.fc = nn.Linear(hidden_chns[-1] * s * s, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def lenet(hidden_chns, ks, pks):\n",
    "    return LeNet(num_classes=100, hidden_chns=hidden_chns, ks=ks, pks=pks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f1ce2-5dd6-48ba-843f-161df36d8d5d",
   "metadata": {},
   "source": [
    "## 2. Define the model and run\n",
    "\n",
    "### Training settings\n",
    "You may modify the `num_epochs` for the fast training or the better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5559de8f-6242-4241-b7f6-2faab5c46328",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20  # training epochs\n",
    "best_acc = 0.0  # best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e59ae-dcb0-452a-b45d-78cc89debe72",
   "metadata": {},
   "source": [
    "### Define the model, optimizer, loss function, learning rate scheduler\n",
    "For step 2, 3, 4, 5, you need to change the network structure of the CNN.\n",
    "\n",
    "`hidden_chns`: a list of the hidden channels of the conv layer.\n",
    "\n",
    "`ks`: a list of the kernel sizes of the conv layer.\n",
    "\n",
    "`pks`: a list of the pooling kernel sizes of the pooling layer. Use `1` if you do not want to use a pooling layer (and it will be an identity function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fcd3b75-035a-4286-94d4-7afb7cab0657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size: 6\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "net = lenet(hidden_chns=[32,64], ks=[3,3], pks=[2,2])\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "net = net.to(device)  # put the model on the specified device(e.g. gpu/cpu)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=mmt, weight_decay=wd) # momentum\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5789a90-3f4e-49c2-8c93-c28a33c105da",
   "metadata": {},
   "source": [
    "### Check the total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2389dbef-e616-4690-97f1-ef374e1d1cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of params 249892\n",
      "Total layers 3\n"
     ]
    }
   ],
   "source": [
    "def count_params(net):\n",
    "    # you can use this function to count amount of your model parameters\n",
    "    import numpy as np\n",
    "    total_params = 0\n",
    "\n",
    "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "        total_params += np.prod(x.data.cpu().numpy().shape)\n",
    "    print(\"Total number of params\", total_params)\n",
    "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
    "\n",
    "\n",
    "count_params(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9285ae-3ead-4b49-8c40-6833919a1591",
   "metadata": {},
   "source": [
    "### Training logs\n",
    "The training logs are saved in the `exp` folder. You can use tensorboard to see the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "972fc6a8-282a-4da7-bb6b-59279a3c2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"exp\"):\n",
    "    os.mkdir(\"exp\")\n",
    "last_train = max([eval(s.split(\"-\")[-1]) for s in os.listdir(\"exp\")] + [0])\n",
    "current_train = last_train + 1\n",
    "save_dir = \"exp/cifar100-{}\".format(current_train)\n",
    "os.makedirs(save_dir)\n",
    "writer = SummaryWriter(save_dir)\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18714981-d3f5-4888-9333-055508713dce",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e08130fc-5109-4174-a32c-2173a69c5dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train:   0%|          | 0/196 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Epoch 0 train: 100%|██████████| 196/196 [00:30<00:00,  6.35it/s, loss=3.791, acc=13.240%, 6620/50000]\n",
      "Epoch 0  test: 100%|██████████| 20/20 [00:04<00:00,  4.52it/s, loss=3.316, acc=21.390%, 2139/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 0 | total time: 36s, test acc: 21.390%, best acc: 21.390%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 train: 100%|██████████| 196/196 [00:31<00:00,  6.23it/s, loss=3.178, acc=23.684%, 11842/50000]\n",
      "Epoch 1  test: 100%|██████████| 20/20 [00:04<00:00,  4.43it/s, loss=2.869, acc=29.920%, 2992/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 1 | total time: 36s, test acc: 29.920%, best acc: 29.920%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 train: 100%|██████████| 196/196 [00:31<00:00,  6.22it/s, loss=2.925, acc=28.500%, 14250/50000]\n",
      "Epoch 2  test: 100%|██████████| 20/20 [00:04<00:00,  4.53it/s, loss=2.704, acc=32.880%, 3288/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 2 | total time: 36s, test acc: 32.880%, best acc: 32.880%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 train: 100%|██████████| 196/196 [00:30<00:00,  6.34it/s, loss=2.803, acc=30.834%, 15417/50000]\n",
      "Epoch 3  test: 100%|██████████| 20/20 [00:04<00:00,  4.87it/s, loss=2.620, acc=35.030%, 3503/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 3 | total time: 36s, test acc: 35.030%, best acc: 35.030%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 train: 100%|██████████| 196/196 [00:31<00:00,  6.26it/s, loss=2.705, acc=32.784%, 16392/50000]\n",
      "Epoch 4  test: 100%|██████████| 20/20 [00:04<00:00,  4.22it/s, loss=2.504, acc=36.800%, 3680/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 4 | total time: 36s, test acc: 36.800%, best acc: 36.800%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 train: 100%|██████████| 196/196 [00:30<00:00,  6.33it/s, loss=2.639, acc=34.140%, 17070/50000]\n",
      "Epoch 5  test: 100%|██████████| 20/20 [00:04<00:00,  4.24it/s, loss=2.473, acc=37.020%, 3702/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 5 | total time: 36s, test acc: 37.020%, best acc: 37.020%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 train: 100%|██████████| 196/196 [00:32<00:00,  6.11it/s, loss=2.586, acc=35.372%, 17686/50000]\n",
      "Epoch 6  test: 100%|██████████| 20/20 [00:04<00:00,  4.24it/s, loss=2.483, acc=37.730%, 3773/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 6 | total time: 37s, test acc: 37.730%, best acc: 37.730%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 train: 100%|██████████| 196/196 [00:31<00:00,  6.25it/s, loss=2.547, acc=36.090%, 18045/50000]\n",
      "Epoch 7  test: 100%|██████████| 20/20 [00:04<00:00,  4.08it/s, loss=2.402, acc=39.610%, 3961/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 7 | total time: 36s, test acc: 39.610%, best acc: 39.610%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 train: 100%|██████████| 196/196 [00:30<00:00,  6.37it/s, loss=2.517, acc=36.442%, 18221/50000]\n",
      "Epoch 8  test: 100%|██████████| 20/20 [00:04<00:00,  4.95it/s, loss=2.429, acc=39.010%, 3901/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | total time: 35s, test acc: 39.010%, best acc: 39.610%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 train: 100%|██████████| 196/196 [00:31<00:00,  6.29it/s, loss=2.459, acc=37.944%, 18972/50000]\n",
      "Epoch 9  test: 100%|██████████| 20/20 [00:03<00:00,  5.18it/s, loss=2.352, acc=39.910%, 3991/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 9 | total time: 35s, test acc: 39.910%, best acc: 39.910%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 train: 100%|██████████| 196/196 [00:31<00:00,  6.26it/s, loss=2.443, acc=38.094%, 19047/50000]\n",
      "Epoch 10  test: 100%|██████████| 20/20 [00:04<00:00,  4.83it/s, loss=2.316, acc=40.950%, 4095/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 10 | total time: 36s, test acc: 40.950%, best acc: 40.950%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 train: 100%|██████████| 196/196 [00:31<00:00,  6.28it/s, loss=2.417, acc=38.544%, 19272/50000]\n",
      "Epoch 11  test: 100%|██████████| 20/20 [00:04<00:00,  4.47it/s, loss=2.360, acc=40.010%, 4001/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | total time: 36s, test acc: 40.010%, best acc: 40.950%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 train: 100%|██████████| 196/196 [00:30<00:00,  6.41it/s, loss=2.406, acc=38.836%, 19418/50000]\n",
      "Epoch 12  test: 100%|██████████| 20/20 [00:03<00:00,  5.02it/s, loss=2.272, acc=42.780%, 4278/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 12 | total time: 35s, test acc: 42.780%, best acc: 42.780%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 train: 100%|██████████| 196/196 [00:30<00:00,  6.35it/s, loss=2.395, acc=39.042%, 19521/50000]\n",
      "Epoch 13  test: 100%|██████████| 20/20 [00:04<00:00,  4.75it/s, loss=2.268, acc=42.200%, 4220/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | total time: 36s, test acc: 42.200%, best acc: 42.780%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 train: 100%|██████████| 196/196 [00:30<00:00,  6.33it/s, loss=2.363, acc=39.850%, 19925/50000]\n",
      "Epoch 14  test: 100%|██████████| 20/20 [00:03<00:00,  5.09it/s, loss=2.273, acc=42.120%, 4212/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | total time: 35s, test acc: 42.120%, best acc: 42.780%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 train: 100%|██████████| 196/196 [00:31<00:00,  6.26it/s, loss=2.339, acc=40.194%, 20097/50000]\n",
      "Epoch 15  test: 100%|██████████| 20/20 [00:03<00:00,  5.10it/s, loss=2.270, acc=41.980%, 4198/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | total time: 36s, test acc: 41.980%, best acc: 42.780%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 train: 100%|██████████| 196/196 [00:31<00:00,  6.24it/s, loss=2.359, acc=40.118%, 20059/50000]\n",
      "Epoch 16  test: 100%|██████████| 20/20 [00:03<00:00,  5.10it/s, loss=2.257, acc=43.050%, 4305/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 16 | total time: 36s, test acc: 43.050%, best acc: 43.050%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 train: 100%|██████████| 196/196 [00:31<00:00,  6.27it/s, loss=2.336, acc=40.420%, 20210/50000]\n",
      "Epoch 17  test: 100%|██████████| 20/20 [00:04<00:00,  4.43it/s, loss=2.208, acc=43.820%, 4382/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 17 | total time: 36s, test acc: 43.820%, best acc: 43.820%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 train: 100%|██████████| 196/196 [00:30<00:00,  6.33it/s, loss=2.317, acc=40.784%, 20392/50000]\n",
      "Epoch 18  test: 100%|██████████| 20/20 [00:03<00:00,  5.16it/s, loss=2.231, acc=43.070%, 4307/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | total time: 35s, test acc: 43.070%, best acc: 43.820%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 train: 100%|██████████| 196/196 [00:31<00:00,  6.29it/s, loss=2.302, acc=41.090%, 20545/50000]\n",
      "Epoch 19  test: 100%|██████████| 20/20 [00:04<00:00,  4.42it/s, loss=2.174, acc=44.250%, 4425/10000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "Epoch 19 | total time: 36s, test acc: 44.250%, best acc: 44.250%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with tqdm.tqdm(enumerate(trainloader), total=len(trainloader)) as t:\n",
    "        t.set_description(f\"Epoch {epoch} train\")\n",
    "        for batch_idx, (inputs, targets) in t:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "#             writer.add_scalars(\n",
    "#                 \"loss\",\n",
    "#                 {\"train\": loss.item()},\n",
    "#                 global_step=epoch * len(trainloader) + batch_idx,\n",
    "#             )\n",
    "\n",
    "            t.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{train_loss/(batch_idx+1):.3f}\",\n",
    "                    \"acc\": f\"{100.*correct/total:.3f}%, {correct}/{total}\",\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    writer.add_scalars(\n",
    "        \"loss\", {\"train\": train_loss/len(trainloader)},\n",
    "        global_step=epoch * len(trainloader)\n",
    "    )\n",
    "    writer.add_scalars(\n",
    "        \"accuracy\", {\"train\": correct/total},\n",
    "        global_step=epoch * len(trainloader)\n",
    "    )\n",
    "\n",
    "\n",
    "# validation\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm.tqdm(enumerate(testloader), total=len(testloader)) as t:\n",
    "            t.set_description(f\"Epoch {epoch}  test\")\n",
    "            for batch_idx, (inputs, targets) in t:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                t.set_postfix(\n",
    "                    {\n",
    "                        \"loss\": f\"{test_loss / (batch_idx + 1):.3f}\",\n",
    "                        \"acc\": f\"{correct*100./total:.3f}%, {correct}/{total}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    writer.add_scalars(\n",
    "        \"loss\", {\"test\": test_loss/len(testloader)},\n",
    "        global_step=epoch * len(trainloader)\n",
    "    )\n",
    "    writer.add_scalars(\n",
    "        \"accuracy\", {\"test\": correct/total},\n",
    "        global_step=epoch * len(trainloader)\n",
    "    )\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.0 * correct / total\n",
    "    if acc > best_acc:\n",
    "        print(\"Saving..\")\n",
    "        state = {\n",
    "            \"net\": net.state_dict(),\n",
    "            \"acc\": acc,\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        torch.save(state, os.path.join(save_dir, \"ckpt.pth\"))\n",
    "        best_acc = acc\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    tic = time.time()\n",
    "    train(epoch)\n",
    "    test_acc = test(epoch)\n",
    "    t = time.time() - tic\n",
    "    print(\n",
    "        f\"Epoch {epoch} | total time: {t:.0f}s, test acc: {test_acc:.3f}%, best acc: {best_acc:.3f}%\"\n",
    "    )\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bceb1c8-9fc0-4343-a49b-15a5ae815121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
