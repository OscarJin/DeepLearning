{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da49b89-1192-4ad7-8efa-7f38b61fafd6",
   "metadata": {},
   "source": [
    "# Homework 2, Step 2. CNN for CIFAR-100\n",
    "\n",
    "In the step 2, you need to try CNN on the CIFAR-100 classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a41c9-3ac0-4313-92aa-05ad4bb6f961",
   "metadata": {},
   "source": [
    "## 1. Prepare the dataset and the model\n",
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704dfb5f-488d-4d4f-813d-45d52316e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import time\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6aa65-3eeb-4020-82d8-035c0ec29a0f",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "**For step 3, 4, 5,** you need to change the value of momentum, weight decay, data augmentation and batch normalization, to see the difference.\n",
    "\n",
    "`mmt`: momentum for the optimizer. Use `0` if you do not want to use the momentum.\n",
    "\n",
    "`wd`: weight decay for the optimizer. Use `0` if you do not want to use the weight decay.\n",
    "\n",
    "`data_augmentation`: whether to use the data augmentation for the training.\n",
    "\n",
    "`use_BN`: wheter to use the batch normalization for the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a13524-df5b-4a31-9aaa-99b0b899eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05 # learning rate\n",
    "opt = 'sgd'\n",
    "batchsize = 256 # training batchsize\n",
    "\n",
    "mmt = 0. # momentum for optimizer\n",
    "wd = 0. # weight_decay for optimizer\n",
    "data_augmentation = False\n",
    "use_BN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b09db2-d22c-4915-b642-40416f6622bd",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "In pytorch, you can use the following API to load the dataset.\n",
    "\n",
    "The RGB mean and std are pre-calculated values for normalizing the data. **Do not modify them**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fbb59-155b-4f57-b834-3eef178ca19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "rgb_std = np.array([0.2023, 0.1994, 0.2010])\n",
    "if data_augmentation:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(rgb_mean, rgb_std),\n",
    "    ])\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(rgb_mean, rgb_std),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(rgb_mean, rgb_std),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batchsize, shuffle=True, num_workers=1)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=500, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de988042-1dff-4cae-ab43-5a190e26fb02",
   "metadata": {},
   "source": [
    "### Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828fe5d-3889-4262-b5f7-a480201901f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "def compute_feature_map_size(ks, pks):\n",
    "    s = 32\n",
    "    for k, pk in zip(ks, pks):\n",
    "        s = s-k+1\n",
    "        s = int(s/pk)\n",
    "    return s\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_chns, ks, pks):\n",
    "        super(LeNet, self).__init__()\n",
    "        in_chns, out_chns = [3, *hidden_chns[:-1]], hidden_chns\n",
    "        layers = []\n",
    "        for ic, oc, k, pk in zip(in_chns, out_chns, ks, pks):\n",
    "            layers.append(nn.Conv2d(ic, oc, kernel_size=k))\n",
    "            if use_BN:\n",
    "                layers.append(nn.BatchNorm2d(oc))\n",
    "            layers.append(nn.MaxPool2d(kernel_size=pk) if pk>1 else Identity()) \n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        s = compute_feature_map_size(ks, pks)\n",
    "        print(f\"feature size: {s}\")\n",
    "\n",
    "        self.fc = nn.Linear(hidden_chns[-1] * s * s, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def lenet(hidden_chns, ks, pks):\n",
    "    return LeNet(num_classes=100, hidden_chns=hidden_chns, ks=ks, pks=pks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f1ce2-5dd6-48ba-843f-161df36d8d5d",
   "metadata": {},
   "source": [
    "## 2. Define the model and run\n",
    "\n",
    "### Training settings\n",
    "You may modify the `num_epochs` for the fast training or the better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559de8f-6242-4241-b7f6-2faab5c46328",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200  # training epochs\n",
    "best_acc = 0.0  # best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e59ae-dcb0-452a-b45d-78cc89debe72",
   "metadata": {},
   "source": [
    "### Define the model, optimizer, loss function, learning rate scheduler\n",
    "For step 2, 3, 4, 5, you need to change the network structure of the CNN.\n",
    "\n",
    "`hidden_chns`: a list of the hidden channels of the conv layer.\n",
    "\n",
    "`ks`: a list of the kernel sizes of the conv layer.\n",
    "\n",
    "`pks`: a list of the pooling kernel sizes of the pooling layer. Use `1` if you do not want to use a pooling layer (and it will be an identity function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd3b75-035a-4286-94d4-7afb7cab0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = lenet(hidden_chns=[16, 16], ks=[3,3], pks=[2,2])\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "net = net.to(device)  # put the model on the specified device(e.g. gpu/cpu)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=mmt, weight_decay=wd) # momentum\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5789a90-3f4e-49c2-8c93-c28a33c105da",
   "metadata": {},
   "source": [
    "### Check the total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389dbef-e616-4690-97f1-ef374e1d1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(net):\n",
    "    # you can use this function to count amount of your model parameters\n",
    "    import numpy as np\n",
    "    total_params = 0\n",
    "\n",
    "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "        total_params += np.prod(x.data.cpu().numpy().shape)\n",
    "    print(\"Total number of params\", total_params)\n",
    "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
    "\n",
    "\n",
    "count_params(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9285ae-3ead-4b49-8c40-6833919a1591",
   "metadata": {},
   "source": [
    "### Training logs\n",
    "The training logs are saved in the `exp` folder. You can use tensorboard to see the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fc6a8-282a-4da7-bb6b-59279a3c2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"exp\"):\n",
    "    os.mkdir(\"exp\")\n",
    "last_train = max([eval(s.split(\"-\")[-1]) for s in os.listdir(\"exp\")] + [0])\n",
    "current_train = last_train + 1\n",
    "save_dir = \"exp/cifar100-{}\".format(current_train)\n",
    "os.makedirs(save_dir)\n",
    "writer = SummaryWriter(save_dir)\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18714981-d3f5-4888-9333-055508713dce",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08130fc-5109-4174-a32c-2173a69c5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with tqdm.tqdm(enumerate(trainloader), total=len(trainloader)) as t:\n",
    "        t.set_description(f\"Epoch {epoch} train\")\n",
    "        for batch_idx, (inputs, targets) in t:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            writer.add_scalars(\n",
    "                \"loss\",\n",
    "                {\"train\": loss.item()},\n",
    "                global_step=epoch * len(trainloader) + batch_idx,\n",
    "            )\n",
    "\n",
    "            t.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{train_loss/(batch_idx+1):.3f}\",\n",
    "                    \"acc\": f\"{100.*correct/total:.3f}%, {correct}/{total}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "# validation\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm.tqdm(enumerate(testloader), total=len(testloader)) as t:\n",
    "            t.set_description(f\"Epoch {epoch}  test\")\n",
    "            for batch_idx, (inputs, targets) in t:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                t.set_postfix(\n",
    "                    {\n",
    "                        \"loss\": f\"{test_loss / (batch_idx + 1):.3f}\",\n",
    "                        \"acc\": f\"{correct*100./total:.3f}%, {correct}/{total}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    writer.add_scalars(\n",
    "        \"loss\", {\"test\": loss.item()}, global_step=epoch * len(trainloader)\n",
    "    )\n",
    "    # Save checkpoint.\n",
    "    acc = 100.0 * correct / total\n",
    "    if acc > best_acc:\n",
    "        print(\"Saving..\")\n",
    "        state = {\n",
    "            \"net\": net.state_dict(),\n",
    "            \"acc\": acc,\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        torch.save(state, os.path.join(save_dir, \"ckpt.pth\"))\n",
    "        best_acc = acc\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    tic = time.time()\n",
    "    train(epoch)\n",
    "    test_acc = test(epoch)\n",
    "    t = time.time() - tic\n",
    "    print(\n",
    "        f\"Epoch {epoch} | total time: {t:.0f}s, test acc: {test_acc:.3f}%, best acc: {best_acc:.3f}%\"\n",
    "    )\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bceb1c8-9fc0-4343-a49b-15a5ae815121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
