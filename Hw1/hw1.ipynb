{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59cff9f-6bed-4fd6-84d0-e8b1f9ca2b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73e1c5-5d18-4e52-9c67-d29ac7dd11e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c923b7-177c-42fc-8e06-a8300b96810b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa25e8d-00fc-4f79-a23e-aa261ca713f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1824af68f884484298502c3da7e0aed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7699b82f16e4e3d81ed83f04973fdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d080671490dd4388aef53339be74e7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c7c17cc7f8465e80e9e32a9436e121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "torch.manual_seed(2022)\n",
    "\n",
    "batchsize = 128  # training batch size\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST(\n",
    "    root='../data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batchsize, shuffle=True, num_workers=1)\n",
    "\n",
    "testset = datasets.MNIST(\n",
    "    root='../data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1000, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24987267-581e-4379-80ce-799e947be61b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Some introductions of the MNIST dataset\n",
    "1. Some quantitative descriptions of the whole dataset \\\n",
    "There are 60000 images in the training set and 10000 images in the test set, the size of single image is (28, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652653b0-3d33-48db-ab60-e8d17d982e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([60000])\n",
      "torch.Size([10000, 28, 28]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(trainset.data.shape, trainset.targets.shape)\n",
    "print(testset.data.shape, testset.targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d748f-89b1-4997-8b21-7c2540e54dbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. Some quantitative descriptions of a single image \\\n",
    "The raw data is taken value in \\[0, 255\\] using integer format. \\\n",
    "For example, you can print the first data in trainset to verify. \\\n",
    "We can also access the data by **dataloader**, and usually take this way. \\\n",
    "However, the data obtained in this way are transformed by specified transformations, and the range of value will also change. \\\n",
    "In this homework, how these transformations works is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3859c32e-f6f9-4c8f-8c7d-f91b426084dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
      "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
      "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
      "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
      "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
      "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
      "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
      "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
      "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
      "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
      "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
      "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
      "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
      "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
      "       dtype=torch.uint8)\n",
      "torch.Size([128, 1, 28, 28]) torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "img1 = trainset.data[0]  # one image from trainset\n",
    "print(img1.shape)\n",
    "print(img1)\n",
    "\n",
    "imgs, labels = next(iter(trainloader))  # get a batch of data from trainloader randomly\n",
    "img2 = imgs[0]\n",
    "print(imgs.shape, img2.shape)\n",
    "# print(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4331cd-8c38-4929-a9d0-6a72762b9e6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "3. Some intuitive illustrations of the dataset \\\n",
    "Here we illustrate the first five images in the trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a801c8b-2601-4a5b-aaa4-b6249777c51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 5\n",
    "fig, ax = plt.subplots(1, n, figsize=(2*n, 2))\n",
    "for i in range(n):\n",
    "    img = trainset.data[i]\n",
    "    ax[i].imshow(img, cmap='gray')\n",
    "    ax[i].set_axis_off()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49af229-c3c8-4fba-8e5f-c311da25544f",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed2fc47-cd6f-4a73-9a57-38bd07b798d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a8044-3967-4aba-9354-2c2b3baa7d5a",
   "metadata": {},
   "source": [
    "### Define Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7426c72a-9801-4bfe-bc9d-6ce95e4a14c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self._saved_tensor = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        pass\n",
    "\n",
    "    def _saved_for_backward(self, tensor):\n",
    "        '''The intermediate results computed during forward stage\n",
    "        can be saved and reused for backward, for saving computation'''\n",
    "\n",
    "        self._saved_tensor = tensor\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: implement this function, return the output\n",
    "        return torch.clamp(input, min=0)\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # TODO: implement this function, return the gradient w.r.t. input\n",
    "        return torch.sign(grad_output+torch.abs(grad_output))\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO, implement this function, return the output\n",
    "        output = 1 / (1+torch.exp(-input))\n",
    "        self.out = output\n",
    "        return output\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # TODO, implement this function, return the gradient w.r.t. input\n",
    "        return grad_output * self.out * (1-self.out)\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # initialization\n",
    "        bound = 1 / math.sqrt(in_features)\n",
    "        self.weight = torch.empty(out_features, in_features)\n",
    "        self.weight.uniform_(-bound, bound)\n",
    "        self.bias = torch.empty(out_features)\n",
    "        self.bias.uniform_(-bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: implement this function, return the output\n",
    "        self.input = input\n",
    "        return input.view(-1, 1) @ self.weight.t() + self.bias\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # 1. return the gradient w.r.t. input\n",
    "        input_grad = grad_output.matmul(self.weight)\n",
    "        # 2. store the gradient w.r.t. parameters in `weight.grad` and `bias.grad`\n",
    "        self.weight.grad = self.input.view(1, -1) @ grad_output\n",
    "        self.bias.grad = grad_output\n",
    "        # TODO: complete this function\n",
    "        return input_grad\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195dff5a-77e6-4310-92d0-2729f17af531",
   "metadata": {},
   "source": [
    "### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae80c50-3ecc-4439-8a8c-de944015c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(object):\n",
    "    def forward(self, input, target):\n",
    "        # TODO: implement this function, return the scalar output\n",
    "        return torch.mean((input-target)**2)\n",
    "        pass\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        # TODO: implement this function, return the gradient w.r.t. input\n",
    "        dx = 2 * (input-target) / torch.numel(input)\n",
    "        return dx\n",
    "        pass\n",
    "\n",
    "\n",
    "class BCELoss(object):\n",
    "    def forward(self, input, target):\n",
    "        # TODO: implement this function, return the scalar output\n",
    "        return torch.mean(-(torch.log(input) * target + torch.log(1-input) * (1-target)))\n",
    "        pass\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        # TODO: implement this function, return the gradient w.r.t. input\n",
    "        return -(target/input - (1-target)/(1-input)) / torch.numel(input)\n",
    "        pass\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(object):\n",
    "    def forward(self, input, target):\n",
    "        # TODO: implement this function, return the scalar output\n",
    "        return torch.mean(-target * torch.log(input))\n",
    "        pass\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        # TODO: implement this function, return the gradient w.r.t. input\n",
    "        return -(target/input) / torch.numel(input)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a2581-97ad-4f4d-9623-1f1299b08eb6",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb41a4-af02-448c-89e0-c5d6c032f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.num_layers = 0\n",
    "\n",
    "    def append(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            out = self.layers[i].forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output\n",
    "        for i in range(self.num_layers - 1, -1, -1):\n",
    "            grad_input = self.layers[i].backward(grad_input)\n",
    "\n",
    "    def parameters(self):\n",
    "        parameters = []\n",
    "        for i in range(self.num_layers):\n",
    "            parameters += self.layers[i].parameters()\n",
    "        return parameters\n",
    "\n",
    "\n",
    "class LogisticRegression(Sequential):\n",
    "    def __init__(self, in_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        # TODO: construct the logistic regression function\n",
    "        pass\n",
    "\n",
    "\n",
    "class TwoLayerNet(Sequential):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dims) -> None:\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "\n",
    "        # TODO: construct a neural network with one hidden layer, using ReLU activation\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07065b6-df3f-4952-b269-311fc5d0de57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Verification\n",
    "We use the tools in **autograd** and **torch.nn** to give a correctness check for the manual implementation. \\\n",
    "Afterwards, we will use these 'tools' in network training directly. \\\n",
    "Here we give a example for CrossEntropyLoss, and you can modify the **input and target** to check other loss functions. \\\n",
    "Next, with the help of loss function, you can check the correctess of layers(Linear, ReLU, Sigmoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c184b5-d31e-4003-a3bd-b6a05d098857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def equal(tensor1, tensor2):\n",
    "    diff = (tensor1 - tensor2).abs().max()\n",
    "    return diff < 1e-5\n",
    "\n",
    "\n",
    "def correctness_test_for_cross_entropy_loss():\n",
    "    loss_standard = nn.CrossEntropyLoss()\n",
    "    loss_ours = CrossEntropyLoss()\n",
    "\n",
    "    # input and target\n",
    "    input = torch.randn(3, 5, requires_grad=True)\n",
    "    target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "    # autograd computation\n",
    "    output_standard = loss_standard(input, target)\n",
    "    output_standard.backward()\n",
    "    grad_standard = input.grad.clone()\n",
    "\n",
    "    # our manual computation\n",
    "    with torch.no_grad():\n",
    "        output_ours = loss_ours.forward(input, target)\n",
    "        grad_ours = loss_ours.backward(input, target)\n",
    "\n",
    "    print(\"CrossEntropyLoss:\")\n",
    "    print(\"Forward Correct\") if equal(output_ours, output_standard) else print(\"Forward Wrong\")\n",
    "    print(\"Backward Correct\") if equal(grad_ours, grad_standard) else print(\"Backward Wrong\")\n",
    "\n",
    "\n",
    "correctness_test_for_cross_entropy_loss()\n",
    "\n",
    "# You can use nn.MSELoss, nn.BCELoss, nn.Linear, nn.Sigmoid, nn.ReLU to do left correctness check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0309f-2957-4730-b292-d663a5a862d5",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9e55a-9cd2-455b-85eb-0eb148b1601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, lr):\n",
    "        assert type(parameters) is list\n",
    "        # Note: In the built-in optimizers like torch.optim.SGD in pytorch, the type of the fisrt parameter 'parameters' is 'generator' rather than 'list'.\n",
    "        # For simplification, we use the 'list' type for 'parameters' here and you can just care about how the optimizer works.\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for p in self.parameters:\n",
    "            # TODO, compute the update vector in gradient descent\n",
    "            upd = TODO\n",
    "            p.data -= upd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7cfc0-69f4-4a39-9d87-759a7c2d8558",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0763046-3f81-42e3-9813-9245f451c7e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe642a-0bc0-4a55-b9fe-0d39f741844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def binarize_label(labels, interested):\n",
    "    return (labels == interested).float()\n",
    "\n",
    "# Training for Logstic Regression\n",
    "def train(model, criterion, train_loader, optimizer, epoch, logging_steps):\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # 0. data preprocessing\n",
    "        inputs = torch.flatten(inputs, start_dim=1)\n",
    "        targets = binarize_label(targets, interested=interested_label)\n",
    "        targets.unsqueeze_(dim=1)\n",
    "\n",
    "        # 1. reset gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. forward computation\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion.forward(outputs, targets)\n",
    "\n",
    "        # 3. backward computation\n",
    "        # compute gradient w.r.t. output\n",
    "        grad_output = criterion.backward(outputs, targets)\n",
    "        # compute gradient w.r.t. parameters\n",
    "        model.backward(grad_output)\n",
    "\n",
    "        # 4. update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # log\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        total = targets.size(0)\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        acc = correct / total\n",
    "        loss_list.append(loss.item())\n",
    "        acc_list.append(acc)\n",
    "        \n",
    "        if batch_idx % logging_steps == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {:.3f}\\tAcc: {:.3f}'.format(\n",
    "                epoch, batch_idx * len(inputs), len(train_loader.dataset),\n",
    "                loss_list[-1], acc_list[-1]))\n",
    "\n",
    "    return loss_list, acc_list\n",
    "\n",
    "# Validation for Logstic Regression\n",
    "def test(model, criterion, test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = torch.flatten(inputs, start_dim=1)\n",
    "        targets = binarize_label(targets, interested=interested_label)\n",
    "        targets.unsqueeze_(dim=1)\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion.forward(outputs, targets)\n",
    "\n",
    "        # log\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        num_batches += 1\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc = correct / total\n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7affa9a-c9c2-418f-be02-f9443226fe03",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591a703-a20d-4575-907e-8431c6eab867",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 # training epochs\n",
    "lr = 0.01 # learning rate\n",
    "logging_steps = 100 # logging batchsize\n",
    "best_acc = 0.0 # best accuracy\n",
    "\n",
    "interested_label = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a427f-a2d4-4c41-8dc1-17621cd882c8",
   "metadata": {},
   "source": [
    "### Model, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f3b26-490e-4ef4-a80e-dbf48c555905",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(in_features=784)\n",
    "criterion = BCELoss()\n",
    "optimizer = SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f9777-bcf3-455b-a57c-0e6d7b01e818",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2db62-71d6-44d7-9580-63acd6b2ca26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Step 1: Logistic Regression\")\n",
    "for epoch in range(num_epochs):\n",
    "    tic = time.time()\n",
    "    train(model, criterion, trainloader, optimizer, epoch, logging_steps)\n",
    "    test_loss, test_acc = test(model, criterion, testloader)\n",
    "    t = time.time() - tic\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    print(f\"Epoch {epoch} | Total Time: {t:.0f}s, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.3f}\")\n",
    "print(f\"Best Accuracy: {best_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf26f5-44c2-42c5-9a5c-6e3b5c7b3245",
   "metadata": {},
   "source": [
    "## Step 2: MLP Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6a8a2-bdae-4933-9571-5ec4b2792b75",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c992508c-36fb-4bb6-8964-43992af84eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def onehot_encoding(targets, num_classes):\n",
    "    eye = torch.eye(num_classes)\n",
    "    onehot_targets = eye[targets]\n",
    "    return onehot_targets\n",
    "\n",
    "# Training for MLP Classification\n",
    "def train(model, criterion, train_loader, optimizer, epoch, logging_steps):\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # 0. data preprocessing\n",
    "        inputs = torch.flatten(inputs, start_dim=1)\n",
    "\n",
    "        # 1. reset gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. forward computation\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion.forward(outputs, targets)\n",
    "\n",
    "        # 3. backward computation\n",
    "        # compute gradient w.r.t. output\n",
    "        grad_output = criterion.backward(outputs, targets)\n",
    "        # compute gradient w.r.t. parameters\n",
    "        model.backward(grad_output)\n",
    "\n",
    "        # 4. update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # log\n",
    "        _, predicted = outputs.max(1)\n",
    "        total = targets.size(0)\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        acc = correct / total\n",
    "        loss_list.append(loss.item())\n",
    "        acc_list.append(acc)\n",
    "        \n",
    "        if batch_idx % logging_steps == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {:.3f}\\tAcc: {:.3f}'.format(\n",
    "                epoch, batch_idx * len(inputs), len(train_loader.dataset),\n",
    "                loss_list[-1], acc_list[-1]))\n",
    "\n",
    "    return loss_list, acc_list\n",
    "\n",
    "# Validation for MLP Classification\n",
    "def test(model, criterion, test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = torch.flatten(inputs, start_dim=1)\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion.forward(outputs, targets)\n",
    "\n",
    "        # log\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        num_batches += 1\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc = correct / total\n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e953a92-3f2d-4858-a1f8-298a5b6ac04d",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe429f3-26ff-411a-8657-6b9d876a58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20 # training epochs\n",
    "lr = 0.01 # learning rate\n",
    "logging_steps = 100 # logging batchsize\n",
    "best_acc = 0.0 # best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d1106-e510-4772-9035-734376879fb9",
   "metadata": {},
   "source": [
    "### Model, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9d70c-f5f6-4a8f-aa1d-312a5af47b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(input_dim=784, num_classes=10, hidden_dims=300)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b38d43-3772-4f6f-8e40-1ecfba2bc05f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b074c3-dea7-495c-872a-77501585b275",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Step 2: MLP Classification\")\n",
    "for epoch in range(num_epochs):\n",
    "    tic = time.time()\n",
    "    train(model, criterion, trainloader, optimizer, epoch, logging_steps)\n",
    "    test_loss, test_acc = test(model, criterion, testloader)\n",
    "    t = time.time() - tic\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    print(f\"Epoch {epoch} | Total Time: {t:.0f}s, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.3f}\")\n",
    "print(f\"Best Accuracy: {best_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34729531-cfc5-4590-9d44-87f9adbdff47",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Training with Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0124d644-c693-4e02-8ae4-4667498e19b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Autograd Introduction\n",
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008db4a7-6495-42b9-b6b6-93cc25c0299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "print(a)\n",
    "b = torch.sin(a)\n",
    "print(b)\n",
    "plt.plot(a.detach(), b.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f08b563-f500-4738-9946-65a1a8294026",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2 * b\n",
    "d = c + 1\n",
    "out = d.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561a764-e21b-480f-803e-65896292a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward() # call the backward to compute the gradient\n",
    "print(a.grad) # the gradient of out w.r.t. a\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32c274-7d59-496f-ab2f-199383b9705f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817b412-abc3-437b-8116-fd0c998aa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b28d11-dfc3-4f7c-8d62-749428c14554",
   "metadata": {},
   "source": [
    "create another tensor Q from a and b.\n",
    "\\begin{align}\n",
    "Q = 3a^3 - b^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74c938-88f2-454b-9319-bb87875325bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc9271f-8b2f-4402-aac7-72989dd3c5f0",
   "metadata": {},
   "source": [
    "then, we have\n",
    "\\begin{align}\n",
    "\\frac{\\partial Q}{\\partial a} = 9a^2 \\\\\n",
    "\\frac{\\partial Q}{\\partial b} = -2b\n",
    "\\end{align}\n",
    "We want to call backward on Q to compute the gradients. \\\n",
    "To do this, we need to explicitly pass a gradient argument in `Q.backward()` because it is a vector, where gradient is a tensor of the same shape as Q, and it represents the gradient of Q w.r.t. itself, i.e.\n",
    "\\begin{align}\n",
    "\\frac{d Q}{d Q} = 1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05039bc1-ef8f-4840-842a-e592ae9618c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e30c74-6b5b-4aa1-922b-7b0e2ad8f0e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62c3b4-be93-4c72-9b2d-361b7143d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20 # training epochs\n",
    "lr = 0.01 # learning rate\n",
    "logging_steps = 100 # logging batchsize\n",
    "best_acc = 0.0 # best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d38f3-3007-4c5e-b470-230845f643bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a3364-4cb4-469d-ae97-d7a483f4fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(input_dim=784, num_classes=10, hidden_dims=300)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# open the requires_grad flag for model parameters to use autograd(or/and computation graph)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41aeac-8489-4798-8048-5bb2ba60c98a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459379a-1343-49c0-a6f6-535f863d8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for MLP Classification with Autograd\n",
    "def train(model, criterion, train_loader, optimizer, epoch, logging_steps):\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # 0. data preprocessing\n",
    "        inputs = torch.flatten(inputs, start_dim=1)\n",
    "\n",
    "        # 1. reset gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. forward computation\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion.forward(outputs, targets)\n",
    "\n",
    "        # 3. backward computation\n",
    "        # the whole backward computation is performed by this single statement\n",
    "        # note that manually defined backward functions are not used\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # log\n",
    "        _, predicted = outputs.max(1)\n",
    "        total = targets.size(0)\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        acc = correct / total\n",
    "        loss_list.append(loss.item())\n",
    "        acc_list.append(acc)\n",
    "        \n",
    "        if batch_idx % logging_steps == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {:.3f}\\tAcc: {:.3f}'.format(\n",
    "                epoch, batch_idx * len(inputs), len(train_loader.dataset),\n",
    "                loss_list[-1], acc_list[-1]))\n",
    "\n",
    "    return loss_list, acc_list\n",
    "\n",
    "# Validation for MLP Classification with Autograd\n",
    "def test(model, criterion, test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = torch.flatten(inputs, start_dim=1)\n",
    "\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion.forward(outputs, targets)\n",
    "\n",
    "            # log\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            num_batches += 1\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc = correct / total\n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91360e76-a67a-4ed3-a675-2fa9f39e54c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592ccb2-411d-4ccb-9137-6d9f93df82f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Step 3: Training with Autograd\")\n",
    "for epoch in range(num_epochs):\n",
    "    tic = time.time()\n",
    "    train(model, criterion, trainloader, optimizer, epoch, logging_steps)\n",
    "    test_loss, test_acc = test(model, criterion, testloader)\n",
    "    t = time.time() - tic\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    print(f\"Epoch {epoch} | Total Time: {t:.0f}s, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.3f}\")\n",
    "print(f\"Best Accuracy: {best_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb20ea3e-de4e-4c44-893a-5e8071a57991",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4: Training with torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc003b7c-0923-48bf-81e7-effa83df3506",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280cf86-a894-4692-a7d8-8f88cab6d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bee038-c594-4b73-8ef5-7a62de1352c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Model\n",
    "`torch.nn` has built-in implementations for the above layers(Linear, ReLU, Sigmoid) and loss functions(MSE, BCE, CrossEntropy) \\\n",
    "Here we give an example about how to use `torch.nn` to construct your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98088b45-a431-483d-98c2-1f382ec1684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03deaa6f-5b9b-4f3b-bef2-13f30bd31fb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then, you need to construct the MLP architecture by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f7fdf-81ce-4c1a-a230-59635c55fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dims=[]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO, construct the MLP architecture with any number of hidden layers, with ReLU activation\n",
    "        # HINT, The ExampleModel is a MLP with two hidden layers\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO, implement this function\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00262b-a947-4ed8-8d60-25afa0d32ec2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94bf50-3b48-4c5e-9cc6-56b53c8a338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20 # training epochs\n",
    "lr = 0.01 # learning rate\n",
    "logging_steps = 100 # logging batchsize\n",
    "best_acc = 0.0 # best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4fcad-df95-4ceb-afa4-22473c0c5c0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29e683-0421-46f5-b0d3-b6cb462f56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_dim=784, num_classes=10, hidden_dims=[300])\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "model = model.to(device) # put the model on the specified device(e.g. gpu/cpu)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6023639f-4a1d-4b4c-88b7-ce9be1d69254",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236b6ac-f079-42e6-990f-e2b5b465ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for MLP Classification with torch.nn\n",
    "def train(model, criterion, train_loader, optimizer, epoch, logging_steps):\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # 0. data preprocessing\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = torch.flatten(inputs, start_dim=1)\n",
    "\n",
    "        # 1. reset gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. forward computation\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # 3. backward computation\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # log\n",
    "        _, predicted = outputs.max(1)\n",
    "        total = targets.size(0)\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        acc = correct / total\n",
    "        loss_list.append(loss.item())\n",
    "        acc_list.append(acc)\n",
    "        \n",
    "        if batch_idx % logging_steps == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {:.3f}\\tAcc: {:.3f}'.format(\n",
    "                epoch, batch_idx * len(inputs), len(train_loader.dataset),\n",
    "                loss_list[-1], acc_list[-1]))\n",
    "\n",
    "    return loss_list, acc_list\n",
    "\n",
    "# Validation for MLP Classification with torch.nn\n",
    "def test(model, criterion, test_loader):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = torch.flatten(inputs, start_dim=1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # log\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            num_batches += 1\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc = correct / total\n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a61d0d5-53cf-4a9f-b6d5-d72421265f5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a83ce-76e1-4120-a698-e6c4dec80043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 4: Training with torch.nn\")\n",
    "for epoch in range(num_epochs):\n",
    "    tic = time.time()\n",
    "    train(model, criterion, trainloader, optimizer, epoch, logging_steps)\n",
    "    test_loss, test_acc = test(model, criterion, testloader)\n",
    "    t = time.time() - tic\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    print(f\"Epoch {epoch} | Total Time: {t:.0f}s, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.3f}\")\n",
    "print(f\"Best Accuracy: {best_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2db13-7df1-4672-81fb-877cfc782161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
